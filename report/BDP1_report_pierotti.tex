\documentclass{article}

\usepackage{graphicx}
\usepackage[margin=2cm,a4paper]{geometry}
\usepackage{tcolorbox}
\tcbuselibrary{listings,skins}
\usepackage{microtype}
\usepackage{placeins}
\usepackage{parskip}
\usepackage{listings}
\lstset{
  language=bash,
  basicstyle=\small\ttfamily,
  columns=flexible,
  breaklines=true,
  showstringspaces=false,
  frame=single
}

\begin{document}

\title{Building IaaS infrastructures on the AWS Cloud}
\author{Saul Pierotti}
\date{\today}

\maketitle

\section{Creation of an HTCondor Cluster for the Alignment of NGS Reads}
The demonstrative infrastructure described in this project consists of an HTCondor cluster of three nodes.
One node is configured as Master Node, while two nodes are configured as Worker Nodes.
The infrastructure can be easily expanded by replicating the Worker Node instances.
The Master Node was not used also as a Worker Node since the performance benefits and cost savings would be marginal.
This was done also to avoid overloading the Master Node, on which the entire cluster depends.
A shared storage space directly attached to the Master Node but available to all the Worker Nodes was also implemented using the distributed file system NFS.

\subsection{Initialization of the Instances on the AWS Cloud}
Worker Nodes and the Master Node were both built on similar machines.
For the Master Node, the \texttt{t2.medium} instance type was used with a 50 Gb SSD as root storage.
For the Worker Nodes, the \texttt{t2.large} instance type was used with a 50 Gb SSD as root storage.
The operating system chosen for both machine types is Ubuntu Server 18.04.4 LTS.
The Master Node and the Worker Nodes were all instantiated in the same availability zone (\texttt{us-east-1a}) so that they would be able to communicate through private IPv4 addresses.
The security group for the instances was configured as follows:

\begin{figure}[!h]
    \center
    \includegraphics[width=\textwidth]{./images/security-group.png}
\end{figure}

All the TCP ports were opened to the other members of the same security group since HTCondor daemons use a dynamically assigned port.
The TCP ports were also needed for the setup of a shared NFS volume.
ICMP ports were opened for accepting incoming \texttt{ping} requests for testing purposes.
TCP port 22 was opened for allowing remote control of the machines via \texttt{ssh}.

\subsection{Configuration of the Master Node}
The PS1 prompt of the Master Node was changed so to make the node easily identifiable from the command line.

\begin{figure}[!h]
    \center
    \includegraphics[width=\textwidth]{./images/master-ps1.png}
\end{figure}

HTCondor was then installed with the following commands:

\begin{lstlisting}
sudo su
wget -qO - https://research.cs.wisc.edu/htcondor/ubuntu/HTCondor-Release.gpg.key | apt-key add - # import the gpg key of HTCondor
echo "deb http://research.cs.wisc.edu/htcondor/ubuntu/8.8/bionic bionic contrib" >> /etc/apt/sources.list # add the repository
echo "deb-src http://research.cs.wisc.edu/htcondor/ubuntu/8.8/bionic bionic contrib" >> /etc/apt/sources.list
apt update
apt install htcondor
systemctl start condor # start and enable the condor service
systemctl enable condor
\end{lstlisting}

The correct proceeding of the installation and the start of the \texttt{condor} service where checked with the following commands:

\begin{figure}[!h]
    \center
    \includegraphics[width=\textwidth]{./images/condor_installed.png}
\end{figure}

The following lines where appended at the end of the main HTCondor configuration file, located at\\
\texttt{/etc/condor/condor\_config}:

\begin{lstlisting}
# Master Node IP
CONDOR_HOST = <Master_Node_private_IP>

# Master Node config 
DAEMON_LIST = COLLECTOR, MASTER, NEGOTIATOR, STARTD
\end{lstlisting}

Finally, the \texttt{condor} service was restarted with the following command:

\begin{lstlisting}
sudo systemctl restart condor
\end{lstlisting}

The NFS server was then implemented in the Master Node.
A new 100 Gb standard magnetic volume was created from the AWS interface and attached to the Master Node.
Fom the server, a primary partiton was initialized on the volume using \texttt{fdisk} and an \texttt{Ext4} file system was created onto it using \texttt{mkfs.ext4}.
The file \texttt{/etc/fstab} of the Master Node was modified so that the machine would mount the volume automatically at boot under the newly created directory \texttt{/data}.
The following line was appended to \texttt{/etc/fstab}:

\begin{lstlisting}
</new_volume/partiton>              /data    ext4   defaults                0 0
\end{lstlisting}

The following commands were then issued, so to install the appropriate packages:

\begin{lstlisting}
sudo apt install nfs-kernel-server
\end{lstlisting}

The following line was appended to the NFS configuration file \texttt{/etc/exports}:

\begin{lstlisting}
/data 172.31.0.0/16(rw,sync,no_wdelay)
\end{lstlisting}

Finally, the owner and group of the shared folder were changed to \texttt{nobody:nogroup} and the permissions of the folder were edited so to grant unlimited access to it:

\begin{lstlisting}
sudo chown nobody:nogroup /data
sudo chmod 777 /data
\end{lstlisting}

The \texttt{/data} folder was so made available to all the Worker Nodes on the address range \texttt{172.31.0.0/16}.
This configuration does not pose a security risk since the Master Node and Worker Nodes machines belong to a Virtual Private Cloud (VPC), and so only machines instantiated on it will be able to access the volume.
Moreover, this configuration grants immediate access to the volume to newly instantiated Worker Nodes.
A mock file was created on the \texttt{/data} folder so to be able to recognize it when mounted.

\begin{lstlisting}
touch /data/this_is_a_shared_NFS_volume
\end{lstlisting}

\subsection{Configuration of the Worker Nodes}
A virtual machine identical to the one used for the Master Node was instantiated.
The PS1 prompt of this first Worker Node was changed so to make the node easily identifiable from the command line.

\begin{figure}[!h]
    \center
    \includegraphics[width=\textwidth]{./images/worker-ps1.png}
\end{figure}

HTCondor was installed in this system with the same procedure used for the Master Node.
Only the \texttt{/etc/condor/condor\_config} file was configured differently, by appending the following lines to it:

\begin{lstlisting}
# Master Node IP
CONDOR_HOST = <Master_Node_private_IP>

# Worker Node config
DAEMON_LIST = MASTER, STARTD

HOSTALLOW_READ = *
HOSTALLOW_WRITE = *
HOSTALLOW_ADMINISTRATOR = *
\end{lstlisting}

On the Worker Node access to the shared NFS volume was then set up.
The following command was issued to install the required packages and enable the respective services:

\begin{lstlisting}
sudo apt install nfs-common
sudo systemctl start nfs-server
sudo systemctl enable nfs-server
\end{lstlisting}

A new directory was then created at \texttt{/data} using the \texttt{mkdir} command.
The \texttt{/etc/fstab} file was edited by appending the following line, so that the shared volume would be automatically mounted at boot under the directory \texttt{/data}:

\begin{lstlisting}
<Master_Node_private_IP>:/data      /data    nfs    defaults                0 0
\end{lstlisting}

It was then verified that the shared volume was accessible from the Worker Node.

\begin{figure}[!h]
    \center
    \includegraphics[width=\textwidth]{./images/nfs_works.png}
\end{figure}

On the Worker Nodes the application BWA was installed with the command:

\begin{lstlisting}
sudo apt install -y bwa
\end{lstlisting}

An image of the virtual machine was taken through the AWS interface.
In this way, the Worker Nodes would be easily replicable when more computational power is needed by instantiating new virtual machines from the image, without the need for manual configuration.

\subsection{Submission of a Test Job to the HTCondor Cluster}
For demonstrating the use of the newly created cluster, a new Worker Node was instantiated from the Worker Node image, so that the cluster would contain the Master Node and 2 Worker Nodes.

Subsequently, a new volume was created in the AWS interface from a snapshot containing test data used for the BDP1 course (\texttt{snap-09ee52d8038fb8094}, BDP1\_2020).
This snapshot contains NGS reads from 3 different patients.
Each patient has a folder with around 500 fasta files, with 1000 reads each.
Part of this data will be used for testing the infrastructure.
The new volume was mounted on the Master Node under \texttt{/data}, substituting the empty volume used before.
The \texttt{/etc/fstab} file was update accordingly and the \texttt{nfs-server} service restarted.
It was then tested that the new volume was accessible from the Worker Nodes.

\begin{figure}[!h]
    \center
    \includegraphics[width=.8\textwidth]{./images/nfs_bdp_works.png}
\end{figure}

The test job consisted in aligning ten fasta files (\texttt{read\_1.fa} to \texttt{read\_10.fa} from patient 1) to the human genome build hg19, also stored on the shared volume.
The BWA alignment tool was used for the scope.
This tool takes advantage of indexing the genome for speeding up the alignment of low-divergent reads.
The index for the hg19 build was already present in the volume snapshot, so there was no need to create it from scratch.
The test fasta files were copied from the shared volume to the home folder of the Master Node.
The test job file \texttt{alignment\_test.job} was created, with the following content:

\begin{lstlisting}
####################################################
################### Alignment Test #################
####################################################


########### The program that will be executed #######

Executable = alignment_test.py
readnum = $(Process)+1
arguments = read_$INT(readnum).fa

############ Input Sandbox  #########################

Input      = read_$INT(readnum).fa
transfer_input_files = read_$INT(readnum).fa

###### Output Sandbox ###############################

Log        = read_$INT(readnum).log
# will contain condor log

Output     = read_$INT(readnum).out
# will contain the standard output

Error      = read_$INT(readnum).error
# will contain the standard error

############## condor control variables #############

should_transfer_files = YES
when_to_transfer_output = ON_EXIT

Universe = vanilla

#####################################################

Queue 10
\end{lstlisting}

The script \texttt{alignment\_test.py} called as executable in \texttt{alignment\_test.job} was the following:

\begin{lstlisting}
#!/usr/bin/python
import sys,os
from timeit import default_timer as timer

start = timer()
dbpath = "/data/BDP1_2020/hg19/"
dbname = "hg19bwaidx"
queryname = sys.argv[1]
out_name = queryname[:-3]
fafile = queryname
samfile = out_name + ".sam"
gzipfile = out_name + ".sam.gz"
saifile = out_name + ".sai"
md5file = out_name + ".md5"

print "Input: ", queryname

command = "bwa aln -t 1 " + dbpath + dbname + " " + fafile + " > " + saifile
print "launching command: " , command
os.system(command)

command = "bwa samse -n 10 " + dbpath + dbname + " " + saifile + " " + fafile + " > " + samfile
print "launching command: " , command
os.system(command)

# Checksums
print "Creating md5sums"
os.system("md5sum " + samfile + " > " + md5file)

print "gzipping out text file"
command = "gzip " + samfile
print "launching command: " , command
os.system(command)

# Transfer files to shared volume and clean the Output Sandbox
print "Moving files and clearing the Output Sandbox"
os.system("mv "+ gzipfile + " /data/outputs/"+ gzipfile)
os.system("mv "+ md5file + " /data/outputs/"+ md5file)
os.system("rm "+ saifile)

execution_time = timer() - start

print "Total execution time: " + str(execution_time)
print "exiting"

exit(0)
\end{lstlisting}

Ten instances of this test job were run on the cluster.
The following outputs of \texttt{condor\_q} and \texttt{condor\_status} were recorded after submission:

\begin{figure}[!h]
    \center
    \includegraphics[width=\textwidth]{./images/condor_q.png}
    \includegraphics[width=\textwidth]{./images/condor_status_busy.png}
\end{figure}
\FloatBarrier

The cluster was able to successfully complete the task and the following output files were produced:

\begin{figure}[!h]
    \center
    \includegraphics[width=\textwidth]{./images/condor_test_out.png}
\end{figure}
\FloatBarrier

The time required to complete the task on a single fasta file ranged from 62.77 to 422.54 seconds, with an average required time of 227.7 seconds.

\begin{figure}[!h]
    \center
    \includegraphics[width=\textwidth]{./images/time_condor.png}
\end{figure}
\FloatBarrier

\subsection{Data Management Model}
The general data management model followed would be that of transferring the executables and the input fasta files using the HTCondor Input Sandbox.
This is because those files are generally small and their transfer will not overload the Master Node.
Moreover, input data are likely to be uploaded dynamically when new sequencing experiments are performed.
This system will allow sending easily new inputs to the cluster.
On the contrary, the large hg19 genome and index files will be made available to the Worker Nodes through the shared NFS volume.
The Output Sandbox will contain the condor log, the standard output, and the standard error.
These files are really small and need to be immediately available to the submitter for inspecting the proceeding of the job.
The aligned reads will instead be compressed and put in the shared volume, to avoid moving large data on the Output Sandbox.

\section{Creation of a Container Image for the Alignment of Next-Generation Sequencing reads}
A containerized version of the BWA application was built using \texttt{docker}.
A container image for the BWA application is already available on DockerHub (\texttt{biocontainers/bwa}), but for educational purposes, a new image was built from scratch.
The Ubuntu docker image was used as a base.
The local docker image \texttt{bwa} was built from the following \texttt{Dockerfile}:

\begin{lstlisting}
FROM ubuntu
RUN apt update
RUN apt install -y bwa
RUN apt install -y python
COPY ./alignment_test_docker.py alignment_test_docker.py
\end{lstlisting}

The container was then run using a bind-mount for the \texttt{/data} volume and starting an interactive shell.

\begin{lstlisting}
docker run -v /data:/data -ti bwa /bin/bash
\end{lstlisting}

The script \texttt{alignment\_test.py} was slightly modified so to omit the final data movement to the shared volume, and renamed \texttt{alignment\_test\_docker.py}.
Inside the container, the following command was run so to align a sample fasta file which had been copied from the shared volume:

\begin{lstlisting}
./alignment_test_docker.py read_1.fa
\end{lstlisting}

The application ran successfully, producing the following output in 213.33 seconds (the container was run on a Ubuntu Server 18.04.4 LTS t2.large AWS virtual machine):

\begin{figure}[!h]
    \center
    \includegraphics[width=\textwidth]{./images/docker_test_out.png}
\end{figure}

\section{Evaluation of the Resources Needed for Solving a Real Use-Case}
A plausible non-trivial use-case that can be addressed using the HTCondor cluster developped in Section 1 is the allignment of a big number of fasta files deriving from a Next-Generation Sequencing (NGS) project to the human reference genome assembly hg19.
It is typical to aim for a coverage of at least 30x in a human Whole-Genome Sequencing (WGS) experiment.
Given that the human genome has a size of about 3.3 billions base pairs, this will result in a raw output from the sequencer of at least 99 billions base pairs.
Considering also the possible occurrence of artifacts and low-quality reads, planning for a raw output of at least 100 billion base pairs is advisable.
A single NGS read of an Illumina MySeq sequencer can have a length of up to 300 base pairs (\url{https://www.illumina.com/systems/sequencing-platforms/miseq.html}).Here I will adopt a read lenght of 150 base pairs, which is recommended for a WGS experiment (\url{https://www.illumina.com/science/technology/next-generation-sequencing/plan-experiments/read-length.html}).
This will result in a total of almost 667 million reads for a single patient.
This amounts to 667000 fasta files of 1000 reads each.
From the tests performed, I roughly determined that the time needed to align one fasta file with 1000 reads of 150 base pairs to the hg19 assembly on a single core of the \texttt{t2.large} machine is 227.3 seconds.
Of course this can vary based on the lenght and composition of the reads themselves.
Thus, completing the alignment of the output of a single WGS experiment would require 151609100 seconds of CPU time on a single core of a \texttt{t2.large} AWS machine.




The scientific challenge proposed during the BDP1 course is an example of a task that can be tackled with the aforementioned infrastructure.
This task consists in aligning 554000 NGS reads from one individual against the hg19 genome.
Given that a single NGS-produced fasta file typically contains 1000 reads, the task consists in aligning 554 fasta files.
Using this rough average, we obtain that aligning the 554 fasta files from one individual will require 125924.2 seconds, around 35 hours of CPU time on a single core like the ones provisioned with the AWS \texttt{t2.large} machine.

If I assume that I will use the cluster detailed in this project, with two Worker Nodes and one Master Node, I will have four cores at my disposal (two per Worker Node).
This means that the time required on the cluster will be of around 9 hours.

I will now describe a breakdown of the expected costs for aligning the reads from a single patient using the AWS EC2 system with On-Demand pricing on the US East N. Virginia availability zone, on a Linux system.
The \texttt{t2.large} machine costs \$ 0.0928 per hour, and the cluster will require two of them for the two Worker Nodes.
The \texttt{t2.medium} machine costs \$ 0.0464 per hour, and the cluster will require one of them for the Master Node.
With an expected time needed for the task of 9 hours, the costs for the computation for one patient will amount to \$ 2.088.
Storage on general purpose SSD EBS devices costs \$ 0.1 per Gb per month.
A 100 Gb disk was used for storing the hg19 genome and index, as well as the reads for the patients.
Therefore, storage will have a cost of \$ 10 per month.
Note that costs for data movements in and out of the AWS system have been omitted.

% 151 bp per read
\end{document}
